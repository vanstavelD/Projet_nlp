{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper et importer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connexion à Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer l'index \"notes\" s'il existe\n",
    "if es.indices.exists(index=\"notes\"):\n",
    "    es.indices.delete(index=\"notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4759/1324505316.py:22: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  es.indices.create(index=\"notes\", body=mapping)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "BadRequestError(400, 'resource_already_exists_exception', 'index [notes/Hv32QN78QDy1jOdPHjh9vQ] already exists')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m      7\u001b[0m mapping \u001b[39m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmappings\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m      9\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mproperties\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     }\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m \u001b[39m# Création de l'index \"notes\" avec le mapping\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m es\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mcreate(index\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnotes\u001b[39;49m\u001b[39m\"\u001b[39;49m, body\u001b[39m=\u001b[39;49mmapping)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.9/site-packages/elasticsearch/_sync/client/utils.py:414\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m api(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.9/site-packages/elasticsearch/_sync/client/indices.py:517\u001b[0m, in \u001b[0;36mIndicesClient.create\u001b[0;34m(self, index, aliases, error_trace, filter_path, human, mappings, master_timeout, pretty, settings, timeout, wait_for_active_shards)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m __body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m     __headers[\u001b[39m\"\u001b[39m\u001b[39mcontent-type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 517\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    518\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPUT\u001b[39;49m\u001b[39m\"\u001b[39;49m, __path, params\u001b[39m=\u001b[39;49m__query, headers\u001b[39m=\u001b[39;49m__headers, body\u001b[39m=\u001b[39;49m__body\n\u001b[1;32m    519\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.9/site-packages/elasticsearch/_sync/client/_base.py:389\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_request\u001b[39m(\n\u001b[1;32m    379\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    380\u001b[0m     method: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[39m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[39m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    390\u001b[0m         method, path, params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders, body\u001b[39m=\u001b[39;49mbody\n\u001b[1;32m    391\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.9/site-packages/elasticsearch/_sync/client/_base.py:320\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    318\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[39m.\u001b[39mget(meta\u001b[39m.\u001b[39mstatus, ApiError)(\n\u001b[1;32m    321\u001b[0m         message\u001b[39m=\u001b[39mmessage, meta\u001b[39m=\u001b[39mmeta, body\u001b[39m=\u001b[39mresp_body\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m \u001b[39m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verified_elasticsearch:\n\u001b[1;32m    326\u001b[0m     \u001b[39m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'resource_already_exists_exception', 'index [notes/Hv32QN78QDy1jOdPHjh9vQ] already exists')"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connexion à Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Mapping de l'index \"notes\"\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"patient_lastname\": {\"type\": \"keyword\"},\n",
    "            \"patient_firstname\": {\"type\": \"keyword\"},\n",
    "            \"text\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"date\": {\"type\": \"date\"},\n",
    "            \"patient_left\": {\"type\": \"boolean\"},\n",
    "            \"emotion\": {\"type\": \"keyword\"},\n",
    "            \"confidence\": {\"type\": \"float\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Création de l'index \"notes\" avec le mapping\n",
    "es.indices.create(index=\"notes\", body=mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des données à partir du jeu de données et de la bibliothèque Faker\n",
    "# Remplacez cette partie par votre propre logique d'importation de données\n",
    "from faker import Faker\n",
    "import csv\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "with open(\"../data/Emotion_final.csv\", \"r\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        document = {\n",
    "            \"patient_lastname\": fake.last_name(),\n",
    "            \"patient_firstname\": fake.first_name(),\n",
    "            \"text\": row[\"Text\"],\n",
    "            \"date\": fake.date(),\n",
    "            \"patient_left\": fake.boolean(),\n",
    "            \"emotion\": \"Model\",  # Remplacez par votre propre logique pour remplir ce champ\n",
    "            \"confidence\": \"Model\"  # Remplacez par votre propre logique pour remplir ce champ\n",
    "        }\n",
    "        es.index(index=\"notes\", body=document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requêtes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche de la répartition des sentiments des textes pour un patient :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Recherche pour un patient spécifique\n",
    "patient_lastname = \"Doe\"\n",
    "patient_firstname = \"John\"\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"match\": {\"patient_lastname\": patient_lastname}},\n",
    "                {\"match\": {\"patient_firstname\": patient_firstname}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"sentiment_distribution\": {\n",
    "            \"terms\": {\"field\": \"emotion.keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result = es.search(index=\"notes\", body=query, size=0)\n",
    "aggregations = result[\"aggregations\"][\"sentiment_distribution\"][\"buckets\"]\n",
    "\n",
    "df_sentiment_distribution = pd.DataFrame(aggregations)\n",
    "print(df_sentiment_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Élaboration d'une matrice de sentiments contradictoires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Liste des sentiments\n",
    "sentiments = [\"happy\", \"sad\", \"angry\", \"calm\"]\n",
    "\n",
    "contradiction_matrix = []\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"match\": {\"emotion\": \"happy\"}},  # Remplacez \"happy\" par le sentiment souhaité\n",
    "                    {\"match\": {\"text\": \"sadness\"}}  # Remplacez \"sadness\" par le mot à rechercher\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = es.search(index=\"notes\", body=query, size=0)\n",
    "    total_documents = result[\"hits\"][\"total\"][\"value\"]\n",
    "\n",
    "    percentage = (total_documents / len(df_sentiment_distribution)) * 100\n",
    "\n",
    "    contradiction_matrix.append(percentage)\n",
    "\n",
    "heatmap_data = pd.DataFrame(contradiction_matrix, index=sentiments, columns=[\"Percentage\"])\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Contradictory Word\")\n",
    "plt.ylabel(\"Sentiment\")\n",
    "plt.title(\"Contradictory Word Percentage by Sentiment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche du nombre de textes correspondants aux différentes étapes du deuil :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_of_grief = [\"denial\", \"anger\", \"bargaining\", \"depression\", \"acceptance\"]\n",
    "\n",
    "for stage in stages_of_grief:\n",
    "    # Recherche pleine\n",
    "    query_full = {\n",
    "        \"query\": {\n",
    "            \"match\": {\"text\": stage}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result_full = es.search(index=\"notes\", body=query_full)\n",
    "    total_documents_full = result_full[\"hits\"][\"total\"][\"value\"]\n",
    "\n",
    "    # Recherche fuzzy\n",
    "    query_fuzzy = {\n",
    "        \"query\": {\n",
    "            \"fuzzy\": {\"text\": {\"value\": stage}}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result_fuzzy = es.search(index=\"notes\", body=query_fuzzy)\n",
    "    total_documents_fuzzy = result_fuzzy[\"hits\"][\"total\"][\"value\"]\n",
    "\n",
    "    print(f\"Stage: {stage}\")\n",
    "    print(f\"Total Documents (Full): {total_documents_full}\")\n",
    "    print(f\"Total Documents (Fuzzy): {total_documents_fuzzy}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche des textes avec différents critères :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"match\": {\"text\": \"good day\"}},  # Doit contenir \"good day\"\n",
    "                {\"term\": {\"patient_left\": False}}  # Les patients sont encore en consultation\n",
    "            ],\n",
    "            \"should\": [\n",
    "                {\"match\": {\"text\": \"to rest\"}}  # Doit contenir \"to rest\" si possible\n",
    "            ],\n",
    "            \"filter\": [\n",
    "                {\"range\": {\"confidence\": {\"gte\": 0.5}}}  # Confidence supérieure ou égale à 0.5\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"sentiment_distribution\": {\n",
    "            \"terms\": {\"field\": \"emotion.keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result = es.search(index=\"notes\", body=query)\n",
    "hits = result[\"hits\"][\"hits\"]\n",
    "\n",
    "df_results = pd.DataFrame(hits)\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
